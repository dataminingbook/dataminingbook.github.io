<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Errata for First Edition | Data Mining and Machine Learning</title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#1f94cf">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://www.dataminingbook.info/errata_first/">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><!-- Font Awesome --><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><meta name="author" content="Zaki&amp;Meira">
<meta property="og:site_name" content="Data Mining and Machine Learning">
<meta property="og:title" content="Errata for First Edition">
<meta property="og:url" content="https://www.dataminingbook.info/errata_first/">
<meta property="og:description" content="This page contains the errata for the first edition. You can
contact us via email if you want to report any errors.

Chapter 1: Data Mining and Analysis

p4, Section 1.3, line 13: as linear combinatio">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-07-12T16:30:54-04:00">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark
bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="../">

            <span id="blog-title">Data Mining and Machine Learning</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../book_html" class="nav-link">ReadOnline</a>
                </li>
<li class="nav-item">
<a href="../errata" class="nav-link">Errata</a>
                </li>
<li class="nav-item">
<a href="../resources" class="nav-link">Resources</a>
                </li>
<li class="nav-item">
<a href="../videos" class="nav-link">Videos</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Errata for First Edition</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>This page contains the errata for the first edition. You can
<a class="reference external" href="mailto:contact@dataminingbook.info">contact us via email</a> if you want to report any errors.</p>
<div class="section" id="chapter-1-data-mining-and-analysis">
<h2>Chapter 1: Data Mining and Analysis</h2>
<ul>
<li><p>p4, Section 1.3, line 13: as linear combination <strong>**should be**</strong> as a linear combination</p></li>
<li><p>p9, Example 1.3, 3rd line from end: <span class="math">\((153)^{1/3}\)</span> <strong>should be</strong> <span class="math">\((152)^{1/3}\)</span></p></li>
<li><p>p9, Example 1.3, last line: <span class="math">\((4^3 + (-1)^3)^{1/3} = (63)^{1/3} = 3.98\)</span> <strong>should be</strong> <span class="math">\((4^3 + |-1|^3)^{1/3} = (65)^{1/3} = 4.02\)</span></p></li>
<li>
<p>p24, Section 1.4.3, last line of subsection Univariate Sample:</p>
<p>where <span class="math">\(f_\mathbf{X}\)</span> is the probability mass or density function for <span class="math">\(\mathbf{X}\)</span></p>
<p><strong>should be</strong></p>
<p>where <span class="math">\(f_X\)</span> is the probability mass or density function for <span class="math">\(X\)</span></p>
</li>
<li><p>p30, Section 1.7, Q1: in (1.5) <strong>should be</strong> in Eq. (1.5)</p></li>
</ul>
</div>
<div class="section" id="chapter-2-numeric-attributes">
<h2>Chapter 2: Numeric Attributes</h2>
<ul>
<li><p>p34, Equation (2.2): <span class="math">\(\hat{F}(x) \ge q\)</span> <strong>should be</strong> <span class="math">\(F(x) \ge q\)</span></p></li>
<li>
<p>p34, Line after Equation (2.2):</p>
<p>That is, the inverse CDF gives the least value of <span class="math">\(X\)</span>, for which
<span class="math">\(q\)</span> fraction of the values are higher, and <span class="math">\(1 - q\)</span>
fraction of the values are lower.</p>
<p><strong>should be</strong></p>
<p>That is, the inverse CDF gives the least value of <span class="math">\(X\)</span>, for which
<span class="math">\(q\)</span> fraction of the values are '''lower''', and <span class="math">\(1 - q\)</span>
fraction of the values are '''higher'''.</p>
</li>
<li><p>p53, Example 2.6, line 1: ... range for <span class="math">\({\tt Income}\)</span> is <span class="math">\(2700-300=2400\)</span> <strong>should be</strong> ... range for <span class="math">\({\tt Income}\)</span> is <span class="math">\(6000-300=5700\)</span></p></li>
<li><p>p55, In Eq (2.32): <span class="math">\(P(-k \le z \le k) = P\bigl(0 \le t \le k/\sqrt{2}\bigr)\)</span> <strong>should be</strong> <span class="math">\(P(-k \le z \le k) = 2 \cdot P\bigl(0 \le t \le k/\sqrt{2}\bigr)\)</span></p></li>
<li><p>p58, Total and Generalized Variance, Line 2: ...product of its eigenvectors <strong>should be</strong> ...product of its eigenvalues</p></li>
<li><p>p58, two lines above Example 2.8: <span class="math">\(tr(\Lambda)\)</span> <strong>should be</strong> <span class="math">\(tr(\mathbf{\Lambda})\)</span></p></li>
<li><p>p61, Q3: <span class="math">\(mu\)</span> <strong>should be</strong> <span class="math">\(\mu\)</span> so that it reads</p></li>
</ul>
<div class="math">
\begin{equation*}
\sum_{i=1}^n (x_i - \mu)^2 = n(\hat{\mu} - \mu)^2 + \sum_{i=1}^n (x_i - \hat{\mu})^2
\end{equation*}
</div>
</div>
<div class="section" id="chapter-3-categorical-attributes">
<h2>Chapter 3: Categorical Attributes</h2>
<ul class="simple">
<li><p>p81, Table 3.6, Attribute value for <span class="math">\(X_2\)</span>: <span class="math">\({\tt Short} ( a_{23})\)</span> <strong>should be</strong> <span class="math">\({\tt Long} ( a_{23})\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-4-graph-data">
<h2>Chapter 4: Graph Data</h2>
<ul>
<li><p>p103, 2 lines above Eq (4.3): <span class="math">\(\gamma_{jk} = 0\)</span> <strong>should be</strong> <span class="math">\(\gamma_{jk}(v_i) = 0\)</span></p></li>
<li><p>p103, Eq (4.3): <span class="math">\(\gamma_{jk}\)</span> <strong>should be</strong> <span class="math">\(\gamma_{jk}(v_i)\)</span></p></li>
<li><p>p103, Example 4.5, last line: <span class="math">\(\gamma_{jk} &gt; 0\)</span> <strong>should be</strong> <span class="math">\(\gamma_{jk}(v_5) &gt; 0\)</span></p></li>
<li>
<p>p104, Example 4.5:</p>
<p><span class="math">\(c(v_5) = \gamma_{18} + \gamma_{24} + \gamma_{27} + \gamma_{28} + \gamma_{38} + \gamma_{46} + \gamma_{48} + \gamma_{67} + \gamma_{68}\)</span></p>
<p><strong>should be</strong></p>
<p><span class="math">\(c(v_5)  = \gamma_{18}(v_5) + \gamma_{24}(v_5) + \gamma_{27}(v_5) + \gamma_{28}(v_5) + \gamma_{38}(v_5) + \gamma_{46}(v_5) + \gamma_{48}(v_5) + \gamma_{67}(v_5) + \gamma_{68}(v_5)\)</span></p>
</li>
<li><p>p107: <span class="math">\(\mathbf{p}_1 = \frac{1}{2} \pmatrix{1\\ 1\\ 2\\ 1\\ 2}\)</span> <strong>should be</strong> <span class="math">\(\mathbf{p}_1 = \frac{1}{2} \pmatrix{1\\ 2\\ 2\\ 1\\ 2}\)</span></p></li>
<li><p>p127, 4th Line after Eq (4.22): initial <span class="math">\(n_0\)</span> edges <strong>should be</strong> initial <span class="math">\(n_0\)</span> nodes</p></li>
</ul>
</div>
<div class="section" id="chapter-5-kernel-methods">
<h2>Chapter 5: Kernel Methods</h2>
<ul>
<li>
<p>p138, Example 5.4:</p>
<p><span class="math">\(\mathbf{\mu}_\phi = \sum_{i=1}^5 \phi(\mathbf{x}_i) =  \sum_{i=1}^5 \mathbf{x}_i\)</span></p>
<p><strong>should be</strong></p>
<p><span class="math">\(\mathbf{\mu}_\phi = \frac{1}{5}\sum_{i=1}^5 \phi(\mathbf{x}_i) =  \frac{1}{5} \sum_{i=1}^5 \mathbf{x}_i\)</span></p>
</li>
<li><p>p140, 7th Line after Eq (5.3): <span class="math">\(\sum_{i=1}^{m_a} \sum_{j=1}^{m_a} \alpha_i \alpha_{\!j} K(\mathbf{x}_i, \mathbf{x})\)</span> <strong>should be</strong> <span class="math">\(\sum_{i=1}^{m_a} \sum_{j=1}^{m_a} \alpha_i \alpha_{\!j} K(\mathbf{x}_i, \mathbf{x}_j)\)</span></p></li>
<li>
<p>p141, 3rd line and 10th Line before Sec 5.1.2: There is an extra left bracket in definition of <span class="math">\(\phi(\mathbf{x})\)</span>, that is,</p>
<p><span class="math">\(\big( ( K(\mathbf{x}_1, \mathbf{x}), ...\)</span> <strong>should be</strong> <span class="math">\(\big( K(\mathbf{x}_1, \mathbf{x}), ...\)</span></p>
</li>
<li><p>p144, 2nd line: <span class="math">\(\int a(\mathbf{x})^2\; d\mathbf{x} &lt; 0\)</span> <strong>should be</strong> <span class="math">\(\int a(\mathbf{x})^2\; d\mathbf{x} &lt; \infty\)</span></p></li>
<li><p>p144, last line: <span class="math">\(\sum_{k=1}^q\)</span> <strong>should be</strong> <span class="math">\(\sum_{k=0}^q\)</span></p></li>
<li><p>p156, Section 5.4.2: all occurrences of path/paths <strong>should be</strong> walk/walks</p></li>
<li><p>p160, Example 5.15L <span class="math">\(\mathbf{S} = -\mathbf{L} = \mathbf{A}-\mathbf{D}\)</span> <strong>should be</strong> <span class="math">\(\mathbf{S} = -\mathbf{L} = \mathbf{A}-\mathbf{\Delta}\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-6-high-dimensional-data">
<h2>Chapter 6: High-dimensional Data</h2>
<ul>
<li>
<p>p164: In the definitions of the hyperball and and hypersphere</p>
<p><span class="math">\(\mathbf{x} = (x_1, x_2, \ldots, x_d)\)</span> <strong>should be</strong> <span class="math">\(\mathbf{x} = (x_1, x_2, \ldots, x_d)^T\)</span></p>
</li>
<li><p>p171: <span class="math">\(\mathbf{0}_d = (0_1,0_2,\ldots,0_d)\)</span> <strong>should be</strong> <span class="math">\(\mathbf{0}_d = (0_1,0_2,\ldots,0_d)^T\)</span></p></li>
<li>
<p>p172, Section 6.6, 1st Line after Eq. (6.11):</p>
<p><span class="math">\(\mu\)</span> in equation <span class="math">\(\mu=\mathbf{0}_d\)</span> <strong>should be</strong> in bold.</p>
</li>
<li>
<p>p178, section Volume in d dimensions:</p>
<p><span class="math">\(x_1 = r \cos\theta_1\cos\theta_2 \cos\theta_3 = r c_2 c_2 c_3\)</span> <strong>should be</strong> <span class="math">\(x_1 = r \cos\theta_1\cos\theta_2 \cos\theta_3 = r c_1 c_2 c_3\)</span></p>
<p><span class="math">\(x_3 = r \cos\theta_1\sin\theta_2 = r c_1 s_1\)</span> <strong>should be</strong> <span class="math">\(x_3 = r \cos\theta_1\sin\theta_2 = r c_1 s_2\)</span></p>
</li>
<li><p>p178, Equation for <span class="math">\(J(\theta_1, \theta_2, \theta_3)\)</span>, Entry in first row, fourth column: <span class="math">\(r c_1 c_2 s_3\)</span>-r c_1 c_2 s_3`</p></li>
<li><p>p207, line 3, Alg 7.2: <span class="math">\(\eta_1, \eta_2, ..., \eta_d\)</span> <strong>should be</strong> <span class="math">\(\eta_1, \eta_2, ..., \eta_n\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-7-dimensionality-reduction">
<h2>Chapter 7: Dimensionality Reduction</h2>
<ul class="simple">
<li><p>p186, line 1: <span class="math">\(\mathbf{a}_r\)</span> is vector <strong>should be</strong> <span class="math">\(\mathbf{a}_r\)</span> is a vector</p></li>
<li><p>p207, line 3, Alg 7.2: <span class="math">\(\eta_1, \eta_2, ..., \eta_d\)</span> <strong>should be</strong> <span class="math">\(\eta_1, \eta_2, ..., \eta_n\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-8-itemset-mining">
<h2>Chapter 8: Itemset Mining</h2>
<ul class="simple">
<li><p>p235, Example 8.13, 2nd last line: <span class="math">\(...,AB(3), AD(4),...\)</span> <strong>should be</strong> <span class="math">\(..., AB(4), AD(3), ...\)</span></p></li>
<li><p>p236, 5th line: <span class="math">\(...,AD(4),...\)</span> <strong>should be</strong> <span class="math">\(..., AD(3),...\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-9-summarizing-itemsets">
<h2>Chapter 9: Summarizing Itemsets</h2>
<ul>
<li><p>p250, 2nd line under '''Generalized Itemsets''': <span class="math">\(k\)</span>-tidsets <strong>should be</strong> <span class="math">\(k\)</span> tidsets</p></li>
<li><p>p250, 4th line from bottom: <span class="math">\(Z = Y \setminus X\)</span> <strong>should be</strong> <span class="math">\(Z = X \setminus Y\)</span></p></li>
<li>
<p>p252, Eq. (9.3) and Eq. (9.4): <span class="math">\(\bigl|X\setminus Y\bigr|\)</span> <strong>should be</strong> <span class="math">\(\bigl|X\setminus W\bigr|\)</span> on the right hand side in both equations, so that they read</p>
<p><span class="math">\(\textbf{Upper Bounds} \bigl(\bigl|X\setminus Y\bigr| \text{is odd} \bigr): sup(X)  \leq\sum_{Y \subseteq W \subset X} -1^{\bigl(\bigl|X\setminus W\bigr|+1\bigr)} sup(W)\)</span></p>
<p><span class="math">\(\textbf{Lower Bounds} \bigl(\bigl|X\setminus Y\bigr| \text{is even}\bigr): sup(X)  \geq\sum_{Y \subseteq W \subset X} -1^{\bigl(\bigl|X\setminus W\bigr|+1\bigr)} sup(W)\)</span></p>
</li>
<li>
<p>p254, Section '''Nonderivable Itemsets''', 1st Equation after line 1: <span class="math">\(\bigl|X\setminus Y\bigr|\)</span> <strong>should be</strong> <span class="math">\(\bigl|X\setminus W\bigr|\)</span> , so that it reads</p>
<p><span class="math">\(\mathit{IE}(Y) = \sum_{Y \subseteq W \subset X}\, -1^{\bigl(\bigl|X\setminus W\bigr|+1\bigr)} \cdot sup(W)\)</span></p>
</li>
</ul>
</div>
<div class="section" id="chapter-10-sequence-mining">
<h2>Chapter 10: Sequence Mining</h2>
<ul class="simple">
<li><p>p264, alg 10.2, line 9: <span class="math">\(\mathbf{P}\)</span> <strong>should be</strong> <span class="math">\(P_a\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-11-graph-pattern-mining">
<h2>Chapter 11: Graph Pattern Mining</h2>
<ul class="simple">
<li><p>p288, sec 11.3, 2nd paragraph, line 6: <span class="math">\(sup(C) = sup(t)\)</span> <strong>should be</strong> <span class="math">\(sup(C') = sup(t)\)</span></p></li>
<li><p>p290, Figure 11.8: The last tuple in the DFS-code for graph <span class="math">\(C_{19}\)</span> <strong>should be</strong> <span class="math">\(\langle 2, 0, a, a \rangle\)</span> and not <span class="math">\(\langle 2, 0, a, b\rangle\)</span></p></li>
<li><p>p292, Algorithm 11.2, Line 14: <span class="math">\(b=\langle u_r, v, L(u_r), L(v), L(u_r, v)\rangle\)</span> <strong>should be</strong> <span class="math">\(b=\langle u_r, v, L(\phi(u_r)), L(\phi(v)), L(\phi(u_r),\phi(v))\rangle\)</span></p></li>
<li><p>p293, Figure 11.9 (c): There there <strong>should be</strong> one more extension for <span class="math">\(\phi_5\)</span>, namely <span class="math">\(\langle 0, 3, a, b\rangle\)</span></p></li>
<li><p>p294, Algorithm 11.3, Line 12: <span class="math">\(N_{G_j}\)</span> <strong>should be</strong> <span class="math">\(N_{G}\)</span></p></li>
<li><p>p295, Algorithm 11.4, Line 0: <span class="math">\(C\)</span> <strong>should be</strong> <span class="math">\(C = \{t_1, t_2, ..., t_k\}\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-12-pattern-and-rule-assessment">
<h2>Chapter 12: Pattern and Rule Assessment</h2>
<ul class="simple">
<li><p>p322 (Alg 12.1) and p326 (Alg 12.2): replace = with <span class="math">\(\gets\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-13-representative-based-clustering">
<h2>Chapter 13: Representative-based Clustering</h2>
<ul class="simple">
<li><p>p343, in 3rd equation: <span class="math">\(P(C_i)\)</span> <strong>should be</strong> <span class="math">\(P(C_1)\)</span></p></li>
<li><p>p335, Algorithm 13.1, line 7: <span class="math">\(\mathbf{\mu}^t_i\)</span> <strong>should be</strong>  <span class="math">\(\mathbf{\mu}^{t-1}_i\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-14-hierarchical-clustering">
<h2>Chapter 14: Hierarchical Clustering</h2>
<ul class="simple">
<li><p>p366, Fig 14.2: (a) <span class="math">\(m=1\)</span>, (b) <span class="math">\(m=2\)</span>, and (c) <span class="math">\(m=3\)</span> <strong>should be</strong> (a) <span class="math">\(n=1\)</span>, (b) <span class="math">\(n=2\)</span>, and  (c) <span class="math">\(n=3\)</span>, respectively.</p></li>
<li><p>p373, sec 14.4: EXERCISES AND PROJECTS <strong>should be</strong> EXERCISES</p></li>
<li><p>p373, Q1, <span class="math">\(SMC(X_i, X_j)\)</span>, <span class="math">\(JC(X_i, X_j)\)</span>, <span class="math">\(RC(X_i, X_j)\)</span> <strong>should be</strong> <span class="math">\(SMC(\mathbf{x}_i, \mathbf{x}_j)\)</span>, <span class="math">\(JC(\mathbf{x}_i, \mathbf{x}_j)\)</span>, <span class="math">\(RC(\mathbf{x}_i, \mathbf{x}_j)\)</span>, respectively.</p></li>
</ul>
</div>
<div class="section" id="chapter-15-density-based-clustering">
<h2>Chapter 15: Density-based Clustering</h2>
<ul class="simple">
<li><p>p385, line after Eq. (15.6): ... having two parts. A vector ...  <strong>should be</strong> ... having two parts: a vector ...</p></li>
<li><p>p387, Alg 15.2, line 20: In the numerator <span class="math">\(K\left(\frac{\mathbf{x}_t - \mathbf{x}_i}{h} \right) \cdot \mathbf{x}_t\)</span> <strong>should be</strong> <span class="math">\(K\left(\frac{\mathbf{x}_t - \mathbf{x}_i}{h} \right) \cdot \mathbf{x}_i\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-16-spectral-and-graph-clustering">
<h2>Chapter 16: Spectral and Graph Clustering</h2>
<ul>
<li>
<p>p411, 2nd last equation: <span class="math">\(\frac{1}{2}p_{rs}\)</span> <strong>should be</strong> <span class="math">\(p_{rs}\)</span> so that it reads</p>
<p><span class="math">\(p_{rs} = \frac{d_r}{2m}\frac{d_s}{2m}  = \frac{d_r d_s}{4m^2}\)</span></p>
</li>
<li><p>p413, Line 5: <span class="math">\(\sum_{j=1}^n \mathbf{d}^T \mathbf{c}_i\)</span> <strong>should be</strong> <span class="math">\(\mathbf{d}^T \mathbf{c}_i\)</span></p></li>
<li><p>p413, Line 10: <span class="math">\((\mathbf{d}_i^T\mathbf{c}_i)^2\)</span> <strong>should be</strong> <span class="math">\((\mathbf{d}^T\mathbf{c}_i)^2\)</span></p></li>
<li><p>p424, Q5: <span class="math">\(\mathbf{c}_n = \frac{1}{\sqrt{n}} \mathbf{1}\)</span> <strong>should be</strong> <span class="math">\(\mathbf{c}_n = \frac{1}{\sqrt{\sum_{i=1}^n d_i}} \mathbf{\Delta}^{1/2}\mathbf{1}\)</span></p></li>
<li><p>p424, Q6 (b): <span class="math">\(\mathbf{K} = \mathbf{M}\)</span> <strong>should be</strong> <span class="math">\(\mathbf{K} = \mathbf{M} + \mathbf{I}\)</span></p></li>
</ul>
</div>
<div class="section" id="chaper-17-clustering-validation">
<h2>Chaper 17: Clustering Validation</h2>
<ul class="simple">
<li><p>p428, Example 17.1, Table below 2nd para: <span class="math">\(n=100\)</span> <strong>should be</strong> <span class="math">\(n=150\)</span> for the total count</p></li>
<li><p>p463, Q10: Add the sentence Assume that the clusters are: <span class="math">\(C_1 = \{a,b, c,d, e\}, C_2 = \{g, i\}, C_3 = \{f,h, j \}, C_4 = \{k\}\)</span>.</p></li>
</ul>
</div>
<div class="section" id="chapter-18-probabilistic-classification">
<h2>Chapter 18: Probabilistic Classification</h2>
<ul class="simple">
<li><p>p472, Table 18.2: 13/50 <strong>should be</strong> 11/50</p></li>
<li><p>p472, Example 18.2, 2nd Para, lines 6 and 7: <span class="math">\(P(c_1|\mathbf{x})\)</span> and  <span class="math">\(P(c_2|\mathbf{x})\)</span> <strong>should be</strong> <span class="math">\(\hat{P}(c_1|\mathbf{x})\)</span> and  <span class="math">\(\hat{P}(c_2|\mathbf{x})\)</span>, respectively.</p></li>
</ul>
</div>
<div class="section" id="chapter-20-linear-discriminant-analysis">
<h2>Chapter 20: Linear Discriminant Analysis</h2>
<ul>
<li>
<p>p503: Example 20.2: There <strong>should be</strong> no transpose operator <span class="math">\(T\)</span> on the mean vectors, i.e.,</p>
<p><span class="math">\(\mathbf{\mu}_1 = \pmatrix{5.01\\3.42}^T \qquad \mathbf{\mu}_2 = \pmatrix{6.26\\2.87}^T \qquad \mathbf{\mu}_1 - \mathbf{\mu}_2= \pmatrix{-1.256\\0.546}^T\)</span></p>
<p><strong>should be</strong></p>
<p><span class="math">\(\mathbf{\mu}_1 = \pmatrix{5.01\\3.42}  \qquad \mathbf{\mu}_2 = \pmatrix{6.26\\2.87} \qquad \mathbf{\mu}_1 - \mathbf{\mu}_2 = \pmatrix{-1.256\\0.546}\)</span></p>
</li>
<li><p>p509, Example 20.4, line 4: ''iris-virginica'' <strong>should be</strong> <span class="math">\({\tt Iris\text{-}versicolor}\)</span></p></li>
<li><p>p512, Q1:  In part (a) <span class="math">\(\mathbf{S}_B\)</span> <strong>should be</strong> <span class="math">\(\mathbf{B}\)</span>, and in (b) <span class="math">\(\mathbf{S}_W\)</span> <strong>should be</strong> <span class="math">\(\mathbf{S}\)</span></p></li>
</ul>
</div>
<div class="section" id="chapter-21-support-vector-machines">
<h2>Chapter 21: Support Vector Machines</h2>
<ul class="simple">
<li><p>p526, 7th line, in <span class="math">\(L_{dual}\)</span>: <span class="math">\((C - \alpha_i + \beta_i)\)</span> <strong>should be</strong> <span class="math">\((C - \alpha_i - \beta_i)\)</span></p></li>
<li><p>p536, Algorithm 21.1, line 15: <span class="math">\(\mathbf{\alpha}_{t+1} = \alpha\)</span> <strong>should be</strong> <span class="math">\(\alpha_{t+1} \gets  \alpha\)</span></p></li>
<li><p>p538, Example 21.8, line 5: homogeneous quadratic kernel <span class="math">\(K(\mathbf{x}_i,\mathbf{x}_j) = ( \mathbf{x}^T_i \mathbf{x}_j)^2\)</span> <strong>should be</strong> inhomogeneous quadratic kernel <span class="math">\(K(\mathbf{x}_i,\mathbf{x}_j) = (1+ \mathbf{x}^T_i \mathbf{x}_j)^2\)</span></p></li>
</ul>
</div>
    </div>
    
        
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script></article><!--End of body content--><footer id="footer">
            Contents Â© 2022         <a href="mailto:contact@dataminingbook.info">Zaki&amp;Meira</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-51598474-1"></script><script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-51598474-1');
</script>
</body>
</html>
