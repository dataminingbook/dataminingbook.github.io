<p>Download the datafile <em>magic04.data</em> from the <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope">UCI Machine Learning
Repository</a>. The
dataset has 10 real attributes, and the last one is simply the class
label, which is categorical, and which you will ignore for this
assignment.</p>
<div class="section" id="principal-component">
<h1>Principal Component</h1>
<p>Compute the dominant eigenvalue and eigenvector of the covariance matrix
<span class="math">\(\mathbf{\Sigma}\)</span> via the power-iteration method. One can compute the
dominant eigen-vector/-value of the covariance matrix iteratively as
follows.</p>
<p>Let</p>
<div class="math">
\begin{equation*}
\mathbf{x}_0 = \begin{pmatrix} 1 \\ 1\\ \vdots \\ 1 \end{pmatrix}
\end{equation*}
</div>
<p>be the starting vector in <span class="math">\(R^d\)</span>, where <span class="math">\(d=10\)</span> is the number of
dimensions.</p>
<p>In each iteration <span class="math">\(i\)</span>, we compute the new vector:</p>
<div class="math">
\begin{equation*}
\mathbf{x}_i = \mathbf{\Sigma} \; \mathbf{x}_{i-1}
\end{equation*}
</div>
<p>We then find the element of <span class="math">\(\mathbf{x}_i\)</span> that  has the maximum
absolute value, say at index <span class="math">\(m\)</span>. For the next round, to avoid
numerical issues with large values, we re-scale <span class="math">\(\mathbf{x}_i\)</span> by
dividing all elements by <span class="math">\(x_{im}\)</span>, so that the largest value is always
1 before we begin the  next  iteration.</p>
<p>To test convergence, you may compute the norm of the difference between
the scaled vectors from the current iteration and the previous one, and
you can stop if this norm falls below some threshold, say 0.0001. That
is, stop if</p>
<div class="math">
\begin{equation*}
\|\mathbf{x}_i - \mathbf{x}_{i-1}\| &lt; 0.0001
\end{equation*}
</div>
<p>For the
final eigen-vector, make sure to normalize it, so that it has unit
length. Also, the ratio <span class="math">\(\frac{x_{im}}{x_{i-1,m}}\)</span> gives you the
largest eigenvalue.</p>
</div>
<div class="section" id="first-two-principal-components">
<h1>First Two Principal Components</h1>
<p>Now compute the first two principal components (PCs) of the covariance
matrix <span class="math">\(\mathbf{\Sigma}\)</span> using a generalization of the above iterative
method.</p>
<p>Let <span class="math">\(\mathbf{X}_0\)</span> be a <span class="math">\(d \times 2\)</span> matrix with two non-zero column
vectors of length <span class="math">\(d\)</span>, with unit length.  We will iteratively multiply
<span class="math">\(\mathbf{X}_0\)</span> with <span class="math">\(\mathbf{\Sigma}\)</span> on the left.</p>
<p>The first column will not be modified, but the second column will be
orthogonalized with respect to the first one by subtracting its
projection along the first column (see section 1.3.3 in chapter 1). That
is, let <span class="math">\(\mathbf{a}\)</span> and <span class="math">\(\mathbf{b}\)</span> denote the first and second
column of <span class="math">\(\mathbf{X}_1\)</span>, where</p>
<div class="math">
\begin{equation*}
\mathbf{X}_1 = \mathbf{\Sigma} \; \mathbf{X}_0
\end{equation*}
</div>
<p>Then we orthogonalize <span class="math">\(\mathbf{b}\)</span> as follows:</p>
<div class="math">
\begin{equation*}
\mathbf{b} = \mathbf{b} - \left({\mathbf{b}^T \mathbf{a} \over \mathbf{a}^T\mathbf{a}}\right) \mathbf{a}
\end{equation*}
</div>
<p>After this <span class="math">\(\mathbf{b}\)</span> is guaranteed to be orthogonal to
<span class="math">\(\mathbf{a}\)</span>. This will yield the final matrix <span class="math">\(\mathbf{X}_1\)</span> with
the two column vectors denoting the current estimates for the first and
second eigenvectors (or the principal components). Before the next
iteration, normalize each column to be unit length, and repeat the whole
process. That is, from <span class="math">\(\mathbf{X}_1\)</span> obtain <span class="math">\(\mathbf{X}_2\)</span> and so
on, until convergence.</p>
<p>To test for convergence,
you can look at the total absolute difference between
<span class="math">\(\mathbf{X}_{i}\)</span> and <span class="math">\(\mathbf{X}_{i-1}\)</span>. If the difference is less than some
threshold, use <span class="math">\(\epsilon=0.001\)</span>, then we stop.</p>
<p>Once you have obtained the two principal components: <span class="math">\(\mathbf{u}_1\)</span> and  <span class="math">\(\mathbf{u}_2\)</span>,
project each of the original points <span class="math">\(\mathbf{x}_i\)</span> onto those two vectors, to obtain the new
projected points in 2D (see <em>Eq. (7.6)</em>). Plot these projected points in the two PC dimensions.</p>
</div>
